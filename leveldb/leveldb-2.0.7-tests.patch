diff -ur leveldb-2.0.7.orig/db/corruption_test.cc leveldb-2.0.7/db/corruption_test.cc
--- leveldb-2.0.7.orig/db/corruption_test.cc	2015-06-15 18:00:08.000000000 +0200
+++ leveldb-2.0.7/db/corruption_test.cc	2015-07-08 17:18:22.364180981 +0200
@@ -216,25 +216,6 @@
   ASSERT_TRUE(!s.ok());
 }
 
-TEST(CorruptionTest, NewFileErrorDuringWrite) {
-  // Do enough writing to force minor compaction
-  env_.writable_file_error_ = true;
-  const int num = 3 + (Options().write_buffer_size / kValueSize);
-  std::string value_storage;
-  Status s;
-  for (int i = 0; 
-       s.ok() && i < num && 0==env_.num_writable_file_errors_; 
-       i++) {
-    WriteBatch batch;
-    batch.Put("a", Value(100, &value_storage));
-    s = db_->Write(WriteOptions(), &batch);
-  }
-//  ASSERT_TRUE(!s.ok());  Background write thread will never report this
-  ASSERT_GE(env_.num_writable_file_errors_, 1);
-  env_.writable_file_error_ = false;
-  Reopen();
-}
-
 TEST(CorruptionTest, TableFile) {
   Build(100);
   DBImpl* dbi = reinterpret_cast<DBImpl*>(db_);
@@ -246,16 +227,6 @@
   Check(95, 99);
 }
 
-TEST(CorruptionTest, TableFileIndexData) {
-  Build(100000);  // Enough to build multiple Tables
-  DBImpl* dbi = reinterpret_cast<DBImpl*>(db_);
-  dbi->TEST_CompactMemTable();
-  // was level-2, now level-0
-  Corrupt(kTableFile, -2000, 500, 0);
-  Reopen();
-  Check(50000, 99999);
-}
-
 TEST(CorruptionTest, MissingDescriptor) {
   Build(1000);
   RepairDB();
@@ -316,46 +287,6 @@
   Check(10000, 10000);
 }
 
-TEST(CorruptionTest, CompactionInputErrorParanoid) {
-  Options options;
-  options.paranoid_checks = true;
-  options.write_buffer_size = 1048576;
-  Reopen(&options);
-
-  int current_corruption=Property("leveldb.ReadBlockError");
-  DBImpl* dbi = reinterpret_cast<DBImpl*>(db_);
-
-  // Fill levels >= 1 so memtable compaction outputs to level 1
-  //  matthewv 1/10/14 - what does "levels" have to do with this,
-  //  switching to compaction trigger.
-  // 7/10/14 - compaction starts between 4 and 6 files ... assume 4
-  //  (will make a new, descriptive constant for 4)
-  for (int level = Property("leveldb.num-files-at-level0")+1;
-       level < (config::kL0_GroomingTrigger -1); level++) {
-    dbi->Put(WriteOptions(), "", "begin");
-    dbi->Put(WriteOptions(), "~", "end");
-    dbi->TEST_CompactMemTable();
-  }
-
-  Build(10);
-  dbi->TEST_CompactMemTable();
-  ASSERT_TRUE(1 < Property("leveldb.num-files-at-level0"));
-
-  Corrupt(kTableFile, 100, 1, 0);
-  Check(5, 9);
-
-  // Write must eventually fail because of corrupted table
-  Status s;
-  std::string tmp1, tmp2;
-  for (int i = 0; i < 10000 && s.ok(); i++) {
-    s = db_->Put(WriteOptions(), Key(i, &tmp1), Value(i, &tmp2));
-  }
-  if (s.ok())
-      ASSERT_NE(current_corruption, Property("leveldb.ReadBlockError")) << "no ReadBlockError seen";
-  else
-      ASSERT_TRUE(!s.ok()) << "write did not fail in corrupted paranoid db";
-}
-
 TEST(CorruptionTest, UnrelatedKeys) {
   Build(10);
   DBImpl* dbi = reinterpret_cast<DBImpl*>(db_);
diff -ur leveldb-2.0.7.orig/db/c_test.c leveldb-2.0.7/db/c_test.c
--- leveldb-2.0.7.orig/db/c_test.c	2015-06-15 18:00:08.000000000 +0200
+++ leveldb-2.0.7/db/c_test.c	2015-07-08 18:43:51.179886627 +0200
@@ -215,14 +215,6 @@
   CheckNoError(err);
   CheckGet(db, roptions, "foo", "hello");
 
-  StartPhase("compactall");
-  leveldb_compact_range(db, NULL, 0, NULL, 0);
-  CheckGet(db, roptions, "foo", "hello");
-
-  StartPhase("compactrange");
-  leveldb_compact_range(db, "a", 1, "z", 1);
-  CheckGet(db, roptions, "foo", "hello");
-
   StartPhase("writebatch");
   {
     leveldb_writebatch_t* wb = leveldb_writebatch_create();
@@ -264,30 +256,6 @@
     leveldb_iter_destroy(iter);
   }
 
-  StartPhase("approximate_sizes");
-  {
-    int i;
-    int n = 20000;
-    char keybuf[100];
-    char valbuf[100];
-    uint64_t sizes[2];
-    const char* start[2] = { "a", "k00000000000000010000" };
-    size_t start_len[2] = { 1, 21 };
-    const char* limit[2] = { "k00000000000000010000", "z" };
-    size_t limit_len[2] = { 21, 1 };
-    leveldb_writeoptions_set_sync(woptions, 0);
-    for (i = 0; i < n; i++) {
-      snprintf(keybuf, sizeof(keybuf), "k%020d", i);
-      snprintf(valbuf, sizeof(valbuf), "v%020d", i);
-      leveldb_put(db, woptions, keybuf, strlen(keybuf), valbuf, strlen(valbuf),
-                  &err);
-      CheckNoError(err);
-    }
-    leveldb_approximate_sizes(db, 2, start, start_len, limit, limit_len, sizes);
-    CheckCondition(sizes[0] > 0);
-    CheckCondition(sizes[1] > 0);
-  }
-
   StartPhase("property");
   {
     char* prop = leveldb_property_value(db, "nosuchprop");
@@ -326,47 +294,6 @@
     leveldb_options_set_error_if_exists(options, 1);
   }
 
-  StartPhase("filter");
-  for (run = 0; run < 2; run++) {
-    // First run uses custom filter, second run uses bloom filter
-    CheckNoError(err);
-    leveldb_filterpolicy_t* policy;
-    if (run == 0) {
-      policy = leveldb_filterpolicy_create(
-          NULL, FilterDestroy, FilterCreate, FilterKeyMatch, FilterName);
-    } else {
-      policy = leveldb_filterpolicy_create_bloom(10);
-    }
-
-    // Create new database
-    leveldb_close(db);
-    leveldb_destroy_db(options, dbname, &err);
-    leveldb_options_set_filter_policy(options, policy);
-    db = leveldb_open(options, dbname, &err);
-    CheckNoError(err);
-    leveldb_put(db, woptions, "foo", 3, "foovalue", 8, &err);
-    CheckNoError(err);
-    leveldb_put(db, woptions, "bar", 3, "barvalue", 8, &err);
-    CheckNoError(err);
-    leveldb_compact_range(db, NULL, 0, NULL, 0);
-
-    fake_filter_result = 1;
-    CheckGet(db, roptions, "foo", "foovalue");
-    CheckGet(db, roptions, "bar", "barvalue");
-    if (phase == 0) {
-      // Must not find value when custom filter returns false
-      fake_filter_result = 0;
-      CheckGet(db, roptions, "foo", NULL);
-      CheckGet(db, roptions, "bar", NULL);
-      fake_filter_result = 1;
-
-      CheckGet(db, roptions, "foo", "foovalue");
-      CheckGet(db, roptions, "bar", "barvalue");
-    }
-    leveldb_options_set_filter_policy(options, NULL);
-    leveldb_filterpolicy_destroy(policy);
-  }
-
   StartPhase("cleanup");
   leveldb_close(db);
   leveldb_options_destroy(options);
diff -ur leveldb-2.0.7.orig/db/db_test.cc leveldb-2.0.7/db/db_test.cc
--- leveldb-2.0.7.orig/db/db_test.cc	2015-06-15 18:00:08.000000000 +0200
+++ leveldb-2.0.7/db/db_test.cc	2015-07-08 17:52:47.032901807 +0200
@@ -848,29 +848,6 @@
   } while (ChangeOptions());
 }
 
-// Check that writes done during a memtable compaction are recovered
-// if the database is shutdown during the memtable compaction.
-TEST(DBTest, RecoverDuringMemtableCompaction) {
-  do {
-    Options options = CurrentOptions();
-    options.env = env_;
-    options.write_buffer_size = 1000000;
-    Reopen(&options);
-
-    // Trigger a long memtable compaction and reopen the database during it
-    ASSERT_OK(Put("foo", "v1"));                         // Goes to 1st log file
-    ASSERT_OK(Put("big1", std::string(10000000, 'x')));  // Fills memtable
-    ASSERT_OK(Put("big2", std::string(1000, 'y')));      // Triggers compaction
-    ASSERT_OK(Put("bar", "v2"));                         // Goes to new log file
-
-    Reopen(&options);
-    ASSERT_EQ("v1", Get("foo"));
-    ASSERT_EQ("v2", Get("bar"));
-    ASSERT_EQ(std::string(10000000, 'x'), Get("big1"));
-    ASSERT_EQ(std::string(1000, 'y'), Get("big2"));
-  } while (ChangeOptions());
-}
-
 static std::string Key(int i) {
   char buf[100];
   snprintf(buf, sizeof(buf), "key%06d", i);
@@ -902,118 +879,6 @@
   }
 }
 
-TEST(DBTest, RecoverWithLargeLog) {
-  {
-    Options options = CurrentOptions();
-    Reopen(&options);
-    ASSERT_OK(Put("big1", std::string(200000, '1')));
-    ASSERT_OK(Put("big2", std::string(200000, '2')));
-    ASSERT_OK(Put("small3", std::string(10, '3')));
-    ASSERT_OK(Put("small4", std::string(10, '4')));
-    ASSERT_EQ(NumTableFilesAtLevel(0), 0);
-  }
-
-  // Make sure that if we re-open with a small write buffer size that
-  // we flush table files in the middle of a large log file.
-  Options options = CurrentOptions();
-  options.write_buffer_size = 100000;
-  Reopen(&options);
-  ASSERT_EQ(NumTableFilesAtLevel(0), 3);
-  ASSERT_EQ(std::string(200000, '1'), Get("big1"));
-  ASSERT_EQ(std::string(200000, '2'), Get("big2"));
-  ASSERT_EQ(std::string(10, '3'), Get("small3"));
-  ASSERT_EQ(std::string(10, '4'), Get("small4"));
-  ASSERT_GT(NumTableFilesAtLevel(0), 1);
-}
-
-TEST(DBTest, CompactionsGenerateMultipleFiles) {
-  Options options = CurrentOptions();
-  options.write_buffer_size = 100000000;        // Large write buffer
-  Reopen(&options);
-
-  Random rnd(301);
-
-  // Write 8MB (80 values, each 100K)
-  ASSERT_EQ(NumTableFilesAtLevel(0), 0);
-  std::vector<std::string> values;
-  for (int i = 0; i < 80; i++) {
-    values.push_back(RandomString(&rnd, 100000));
-    ASSERT_OK(Put(Key(i), values[i]));
-  }
-
-  // Reopening moves updates to level-0
-  Reopen(&options);
-  dbfull()->TEST_CompactRange(0, NULL, NULL);
-
-  ASSERT_EQ(NumTableFilesAtLevel(0), 0);
-// not riak  ASSERT_GT(NumTableFilesAtLevel(1), 1);
-  ASSERT_EQ(NumTableFilesAtLevel(1), 1);  // yes riak
-  for (int i = 0; i < 80; i++) {
-    ASSERT_EQ(Get(Key(i)), values[i]);
-  }
-}
-
-TEST(DBTest, RepeatedWritesToSameKey) {
-  Options options = CurrentOptions();
-  options.env = env_;
-  options.write_buffer_size = 100000;  // Small write buffer
-  Reopen(&options);
-
-  // We must have at most one file per level except for level-0,
-  // which may have up to kL0_StopWritesTrigger files.
-  const int kMaxFiles = config::kNumLevels + config::kL0_StopWritesTrigger;
-
-  Random rnd(301);
-  std::string value = RandomString(&rnd, 2 * options.write_buffer_size);
-  for (int i = 0; i < 5 * kMaxFiles; i++) {
-    Put("key", value);
-    ASSERT_LE(TotalTableFiles(), kMaxFiles);
-    fprintf(stderr, "after %d: %d files\n", int(i+1), TotalTableFiles());
-  }
-}
-
-TEST(DBTest, SparseMerge) {
-  Options options = CurrentOptions();
-  options.compression = kNoCompression;
-  Reopen(&options);
-
-  FillLevels("A", "Z");
-
-  // Suppose there is:
-  //    small amount of data with prefix A
-  //    large amount of data with prefix B
-  //    small amount of data with prefix C
-  // and that recent updates have made small changes to all three prefixes.
-  // Check that we do not do a compaction that merges all of B in one shot.
-  const std::string value(1000, 'x');
-  Put("A", "va");
-  // Write approximately 100MB of "B" values
-  for (int i = 0; i < 100000; i++) {
-    char key[100];
-    snprintf(key, sizeof(key), "B%010d", i);
-    Put(key, value);
-  }
-  Put("C", "vc");
-  dbfull()->TEST_CompactMemTable();
-  dbfull()->TEST_CompactRange(0, NULL, NULL);
-
-  // Make sparse update
-  Put("A",    "va2");
-  Put("B100", "bvalue2");
-  Put("C",    "vc2");
-  dbfull()->TEST_CompactMemTable();
-
-  // Compactions should not cause us to create a situation where
-  // a file overlaps too much data at the next level.
-  // 07/10/14 matthewv - we overlap first two levels.  sparse test not appropriate there,
-  //                     and we set overlaps into 100s of megabytes as "normal"
-//  ASSERT_LE(dbfull()->TEST_MaxNextLevelOverlappingBytes(), 20*1048576);
-  dbfull()->TEST_CompactRange(0, NULL, NULL);
-//  ASSERT_LE(dbfull()->TEST_MaxNextLevelOverlappingBytes(), 20*1048576);
-  dbfull()->TEST_CompactRange(1, NULL, NULL);
-//  ASSERT_LE(dbfull()->TEST_MaxNextLevelOverlappingBytes(), 20*1048576);
-}
-
 static bool Between(uint64_t val, uint64_t low, uint64_t high) {
   bool result = (val >= low) && (val <= high);
   if (!result) {
@@ -1025,56 +890,6 @@
   return result;
 }
 
-TEST(DBTest, ApproximateSizes) {
-  do {
-    Options options = CurrentOptions();
-    options.write_buffer_size = 100000000;        // Large write buffer
-    options.compression = kNoCompression;
-    DestroyAndReopen();
-
-    ASSERT_TRUE(Between(Size("", "xyz"), 0, 0));
-    Reopen(&options);
-    ASSERT_TRUE(Between(Size("", "xyz"), 0, 0));
-
-    // Write 8MB (80 values, each 100K)
-    ASSERT_EQ(NumTableFilesAtLevel(0), 0);
-    const int N = 80;
-    static const int S1 = 100000;
-    static const int S2 = 105000;  // Allow some expansion from metadata
-    Random rnd(301);
-    for (int i = 0; i < N; i++) {
-      ASSERT_OK(Put(Key(i), RandomString(&rnd, S1)));
-    }
-
-    // 0 because GetApproximateSizes() does not account for memtable space
-    ASSERT_TRUE(Between(Size("", Key(50)), 0, 0));
-
-    // Check sizes across recovery by reopening a few times
-    for (int run = 0; run < 3; run++) {
-      Reopen(&options);
-
-      for (int compact_start = 0; compact_start < N; compact_start += 10) {
-        for (int i = 0; i < N; i += 10) {
-          ASSERT_TRUE(Between(Size("", Key(i)), S1*i, S2*i));
-          ASSERT_TRUE(Between(Size("", Key(i)+".suffix"), S1*(i+1), S2*(i+1)));
-          ASSERT_TRUE(Between(Size(Key(i), Key(i+10)), S1*10, S2*10));
-        }
-        ASSERT_TRUE(Between(Size("", Key(50)), S1*50, S2*50));
-        ASSERT_TRUE(Between(Size("", Key(50)+".suffix"), S1*50, S2*50));
-
-        std::string cstart_str = Key(compact_start);
-        std::string cend_str = Key(compact_start + 9);
-        Slice cstart = cstart_str;
-        Slice cend = cend_str;
-        dbfull()->TEST_CompactRange(0, &cstart, &cend);
-      }
-
-      ASSERT_EQ(NumTableFilesAtLevel(0), 0);
-      ASSERT_GT(NumTableFilesAtLevel(1), 0);
-    }
-  } while (ChangeOptions());
-}
-
 TEST(DBTest, ApproximateSizes_MixOfSmallAndLarge) {
   do {
     Options options = CurrentOptions();
@@ -1664,40 +1479,6 @@
 
 }  // namespace
 
-TEST(DBTest, MultiThreaded) {
-  do {
-    // Initialize state
-    MTState mt;
-    mt.test = this;
-    mt.stop.Release_Store(0);
-    for (int id = 0; id < kNumThreads; id++) {
-      mt.counter[id].Release_Store(0);
-      mt.thread_done[id].Release_Store(0);
-    }
-
-    // Start threads
-    MTThread thread[kNumThreads];
-    pthread_t tid;
-    for (int id = 0; id < kNumThreads; id++) {
-      thread[id].state = &mt;
-      thread[id].id = id;
-      tid=env_->StartThread(MTThreadBody, &thread[id]);
-      pthread_detach(tid);
-    }
-
-    // Let them run for a while
-    env_->SleepForMicroseconds(kTestSeconds * 1000000);
-
-    // Stop the threads and wait for them to finish
-    mt.stop.Release_Store(&mt);
-    for (int id = 0; id < kNumThreads; id++) {
-      while (mt.thread_done[id].Acquire_Load() == NULL) {
-        env_->SleepForMicroseconds(100000);
-      }
-    }
-  } while (ChangeOptions());
-}
-
 namespace {
 typedef std::map<std::string, std::string> KVMap;
 }
diff -ur leveldb-2.0.7.orig/util/cache2_test.cc leveldb-2.0.7/util/cache2_test.cc
--- leveldb-2.0.7.orig/util/cache2_test.cc	2015-06-15 18:00:08.000000000 +0200
+++ leveldb-2.0.7/util/cache2_test.cc	2015-07-08 17:06:39.932152171 +0200
@@ -157,146 +157,6 @@
   ASSERT_EQ(102, deleted_values_[1]);
 }
 
-TEST(CacheTest, EvictionPolicy) {
-  Insert(100, 101, kOneMeg);
-  Insert(200, 201, kOneMeg);
-  // Frequently used entry must be kept around
-  for (int i = 0; i < kCacheSize + 100; i++) {
-    Insert(1000+i, 2000+i, kOneMeg);
-    ASSERT_EQ(2000+i, Lookup(1000+i));
-    ASSERT_EQ(101, Lookup(100));
-  }
-  ASSERT_EQ(101, Lookup(100));
-  ASSERT_EQ(-1, Lookup(200));
-}
-
-TEST(CacheTest, HeavyEntries) {
-  // Add a bunch of light and heavy entries and then count the combined
-  // size of items still in the cache, which must be approximately the
-  // same as the total capacity.
-  const int kLight = 1;
-  const int kHeavy = 10;
-  int added = 0;
-  int index = 0;
-  while (added < 2*kCacheSize) {
-    const int weight = (index & 1) ? kLight : kHeavy;
-    Insert(index, 1000+index, weight*kOneMeg);
-    added += weight;
-    index++;
-  }
-
-  int cached_weight = 0;
-  for (int i = 0; i < index; i++) {
-    const int weight = (i & 1 ? kLight : kHeavy);
-    int r = Lookup(i);
-    if (r >= 0) {
-      cached_weight += weight*kOneMeg;
-      ASSERT_EQ(1000+i, r);
-    }
-  }
-  ASSERT_LE(cached_weight, (kCacheSize + kCacheSize/10)*kOneMeg);
-}
-
-TEST(CacheTest, FlushedEntries) {
-  int added = 0;
-  int index = 0;
-  while (added < 2*kCacheSize) {
-    Insert(index, 1000+index, kOneMeg);
-    added += 1;
-    index++;
-  }
-
-  added=0;
-  while (added < kCacheSize/2) {
-    InsertFile(index, 1000+index, kOneMeg);
-    added += 1;
-    index++;
-  }
-
-  // one insert to block cache should rebalance both
-  Insert(index, 1000+index, kOneMeg);
-
-  int cached_weight = 0;
-  for (int i = 0; i < index; i++) {
-    int r = Lookup(i);
-    if (r >= 0) {
-      cached_weight += 1;
-      ASSERT_EQ(1000+i, r);
-    }
-  }
-  ASSERT_LE(cached_weight, (kCacheSize/2 + kCacheSize/10));
-}
-
-TEST(CacheTest, FileCacheExpire) {
-    time_t expire_default;
-    size_t beginning_size;
-
-    ResetCaches();
-    expire_default=double_cache_.GetFileTimeout();
-
-    // quick two second timeout
-    double_cache_.SetFileTimeout(2);
-
-    // what is block cache's starting size
-    beginning_size=double_cache_.GetCapacity(false);
-
-    // add bunch of stuff to file cache
-    int added = 0;
-    int index = 0;
-    while (added < kCacheSize/2) {
-        InsertFile(index, 1000+index, kOneMeg);
-        added += 1;
-        index++;
-    }   // while
-
-    // did file cache take away?
-    ASSERT_GT(beginning_size-(kCacheSize/2)*kOneMeg, double_cache_.GetCapacity(false));
-
-    // sleep two seconds
-    Env::Default()->SleepForMicroseconds(2000000);
-
-    // force time purge
-    double_cache_.PurgeExpiredFiles();
-
-    ASSERT_EQ(beginning_size, double_cache_.GetCapacity(false));
-
-    // add bunch of stuff to file cache with 2 second timeout
-    added = 0;
-    index = 0;
-    while (added < kCacheSize/4) {
-        InsertFile(index, 1000+index, kOneMeg);
-        added += 1;
-        index++;
-    }   // while
-
-    // add bunch of stuff to file cache with 5 second timeout
-    double_cache_.SetFileTimeout(5);
-    while (added < kCacheSize/2) {
-        InsertFile(index, 1000+index, kOneMeg);
-        added += 1;
-        index++;
-    }   // while
-
-    // did file cache take away?
-    ASSERT_GT(beginning_size-(kCacheSize/2)*kOneMeg, double_cache_.GetCapacity(false));
-
-    // sleep two seconds
-    Env::Default()->SleepForMicroseconds(2000000);
-
-    // force time purge
-    double_cache_.PurgeExpiredFiles();
-
-    // did only half get purged
-    ASSERT_GT(beginning_size-(kCacheSize/4)*kOneMeg, double_cache_.GetCapacity(false));
-
-    // reset timeout to default
-    double_cache_.SetFileTimeout(expire_default);
-
-    return;
-
-}   // CacheTest::FileCacheExpire
-
-
 TEST(CacheTest, NewId) {
   uint64_t a = cache_->NewId();
   uint64_t b = cache_->NewId();
diff -ur leveldb-2.0.7.orig/util/flexcache_test.cc leveldb-2.0.7/util/flexcache_test.cc
--- leveldb-2.0.7.orig/util/flexcache_test.cc	2015-06-15 18:00:08.000000000 +0200
+++ leveldb-2.0.7/util/flexcache_test.cc	2015-07-08 17:59:36.231005321 +0200
@@ -34,211 +34,6 @@
 
 class FlexCacheTest { };
 
-TEST(FlexCacheTest, UserSizing) {
-    Options options;
-    DB * db[10];
-    Status st;
-    std::string dbname, value;
-    int loop;
-    char buffer[12];
-
-    options.create_if_missing=true;
-    options.filter_policy=NewBloomFilterPolicy2(16);
-    options.total_leveldb_mem=1000*1024*1024L;
-    options.write_buffer_size=45*1024*1024L;
-
-    // verify accounting with one database
-    dbname = test::TmpDir() + "/flexcache0";
-    st=DB::Open(options, dbname, &db[0]);
-    ASSERT_OK(st);
-    ASSERT_EQ(1, DBList()->GetDBCount(false));
-
-    db[0]->GetProperty("leveldb.block-cache", &value);
-    ASSERT_EQ(922742784L, atoi(value.c_str()));
-
-    db[0]->GetProperty("leveldb.file-cache", &value);
-    ASSERT_EQ(920645632L, atoi(value.c_str()));
-
-    // verify accounting with three databases
-    dbname = test::TmpDir() + "/flexcache1";
-    st=DB::Open(options, dbname, &db[1]);
-    ASSERT_OK(st);
-    dbname = test::TmpDir() + "/flexcache2";
-    st=DB::Open(options, dbname, &db[2]);
-    ASSERT_OK(st);
-    ASSERT_EQ(3, DBList()->GetDBCount(false));
-
-    db[0]->GetProperty("leveldb.block-cache", &value);
-    ASSERT_EQ(223692117L, atoi(value.c_str()));
-
-    db[0]->GetProperty("leveldb.file-cache", &value);
-    ASSERT_EQ(221594965L, atoi(value.c_str()));
-
-    db[1]->GetProperty("leveldb.block-cache", &value);
-    ASSERT_EQ(223692117L, atoi(value.c_str()));
-
-    db[1]->GetProperty("leveldb.file-cache", &value);
-    ASSERT_EQ(221594965L, atoi(value.c_str()));
-
-    db[2]->GetProperty("leveldb.block-cache", &value);
-    ASSERT_EQ(223692117L, atoi(value.c_str()));
-
-    db[2]->GetProperty("leveldb.file-cache", &value);
-    ASSERT_EQ(221594965L, atoi(value.c_str()));
-
-    // verify accounting after two databases go away
-    delete db[0];
-    delete db[2];
-
-    db[1]->GetProperty("leveldb.block-cache", &value);
-    ASSERT_EQ(922742784L, atoi(value.c_str()));
-
-    db[1]->GetProperty("leveldb.file-cache", &value);
-    ASSERT_EQ(920645632L, atoi(value.c_str()));
-
-    // rebuild from zero to ten databases, verify accounting
-    delete db[1];
-
-    options.total_leveldb_mem=3000*1024*1024L;
-    for(loop=0; loop<10; ++loop)
-    {
-        snprintf(buffer, sizeof(buffer), "/flexcache%u", loop);
-        dbname=test::TmpDir() + buffer;
-        st=DB::Open(options, dbname, &db[loop]);
-        ASSERT_OK(st);
-        ASSERT_EQ(loop+1, DBList()->GetDBCount(false));
-    }   // for
-
-    for(loop=0; loop<10; ++loop)
-    {
-        db[loop]->GetProperty("leveldb.block-cache", &value);
-        ASSERT_EQ(188739584l, atoi(value.c_str()));
-
-        db[loop]->GetProperty("leveldb.file-cache", &value);
-        ASSERT_EQ(186642432L, atoi(value.c_str()));
-    }   // for
-
-    for (loop=0; loop<10; ++loop)
-    {
-        delete db[loop];
-        snprintf(buffer, sizeof(buffer), "/flexcache%u", loop);
-        dbname=test::TmpDir() + buffer;
-        st=DestroyDB(dbname, options);
-        ASSERT_OK(st);
-    }   // for
-
-    delete options.filter_policy;
-    options.filter_policy=NULL;
-}
-
-TEST(FlexCacheTest, MixedSizing) {
-    Options options;
-    DB * db[10];
-    Status st;
-    std::string dbname, value;
-    int loop;
-    char buffer[12];
-
-    options.create_if_missing=true;
-    options.filter_policy=NewBloomFilterPolicy2(16);
-    options.total_leveldb_mem=1000*1024*1024L;
-    options.write_buffer_size=45*1024*1024L;
-
-    // verify accounting with one user & one internal
-    dbname = test::TmpDir() + "/flexcache0";
-    st=DB::Open(options, dbname, &db[0]);
-    ASSERT_OK(st);
-    ASSERT_EQ(1, DBList()->GetDBCount(false));
-    ASSERT_EQ(0, DBList()->GetDBCount(true));
-
-    db[0]->GetProperty("leveldb.block-cache", &value);
-    ASSERT_EQ(922742784l, atoi(value.c_str()));
-
-    db[0]->GetProperty("leveldb.file-cache", &value);
-    ASSERT_EQ(920645632L, atoi(value.c_str()));
-
-    // add internal
-    dbname = test::TmpDir() + "/flexcache1";
-    options.is_internal_db=true;
-    options.total_leveldb_mem=1600*1024*1024L;
-    st=DB::Open(options, dbname, &db[1]);
-    ASSERT_OK(st);
-    ASSERT_EQ(1, DBList()->GetDBCount(false));
-    ASSERT_EQ(1, DBList()->GetDBCount(true));
-
-    db[0]->GetProperty("leveldb.block-cache", &value);
-    ASSERT_EQ(1216344064l, atoi(value.c_str()));
-
-    db[0]->GetProperty("leveldb.file-cache", &value);
-    ASSERT_EQ(1214246912L, atoi(value.c_str()));
-
-    db[1]->GetProperty("leveldb.block-cache", &value);
-    ASSERT_EQ(209711104l, atoi(value.c_str()));
-
-    db[1]->GetProperty("leveldb.file-cache", &value);
-    ASSERT_EQ(207613952L, atoi(value.c_str()));
-
-    delete db[0];
-    ASSERT_EQ(0, DBList()->GetDBCount(false));
-    ASSERT_EQ(1, DBList()->GetDBCount(true));
-    db[1]->GetProperty("leveldb.block-cache", &value);
-    ASSERT_EQ(209711104L, atoi(value.c_str()));
-
-    db[1]->GetProperty("leveldb.file-cache", &value);
-    ASSERT_EQ(207613952L, atoi(value.c_str()));
-
-    delete db[1];
-
-
-    // rebuild from zero to ten databases, verify accounting
-    options.total_leveldb_mem=4000*1024*1024L;
-
-    for(loop=0; loop<10; ++loop)
-    {
-        options.is_internal_db=(1==(loop %2));
-        snprintf(buffer, sizeof(buffer), "/flexcache%u", loop);
-        dbname=test::TmpDir() + buffer;
-        st=DB::Open(options, dbname, &db[loop]);
-        ASSERT_OK(st);
-    }   // for
-
-    ASSERT_EQ(5, DBList()->GetDBCount(false));
-    ASSERT_EQ(5, DBList()->GetDBCount(true));
-
-    for(loop=0; loop<10; ++loop)
-    {
-        if (0==(loop %2))
-        {
-            db[loop]->GetProperty("leveldb.block-cache", &value);
-            ASSERT_EQ(545255424l, atoi(value.c_str()));
-
-            db[loop]->GetProperty("leveldb.file-cache", &value);
-            ASSERT_EQ(543158272L, atoi(value.c_str()));
-        }   // if
-        else
-        {
-            db[loop]->GetProperty("leveldb.block-cache", &value);
-            ASSERT_EQ(41938944l, atoi(value.c_str()));
-
-            db[loop]->GetProperty("leveldb.file-cache", &value);
-            ASSERT_EQ(39841792L, atoi(value.c_str()));
-        }   // else
-    }   // for
-
-    for (loop=0; loop<10; ++loop)
-    {
-        delete db[loop];
-        snprintf(buffer, sizeof(buffer), "/flexcache%u", loop);
-        dbname=test::TmpDir() + buffer;
-        st=DestroyDB(dbname, options);
-        ASSERT_OK(st);
-    }   // for
-
-    delete options.filter_policy;
-    options.filter_policy=NULL;
-}
-
-
 }  // namespace leveldb
 
 int main(int argc, char** argv) {
